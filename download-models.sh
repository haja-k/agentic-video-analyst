#!/bin/bash

# Quick model download script for M2 Mac
# Downloads the minimum viable models for a working demo

set -e

echo "üì• Downloading AI Models for Demo..."
echo ""

cd backend/models

echo "Choose your model setup:"
echo ""
echo "Option 1: Llama 3.1 8B (RECOMMENDED - proven & public)"
echo "  - Strong instruction following, good for orchestration"
echo "  - ~4.9GB download, ~5-6GB RAM usage"
echo "  - Vision via BLIP-2 (auto-downloads)"
echo ""
echo "Option 2: Mistral 7B Instruct (Fast alternative)"
echo "  - Excellent instruction following and JSON generation"
echo "  - ~4.4GB download, ~5GB RAM usage"
echo ""
echo "Option 3: Llama 3.2 3B (Smaller/faster)"
echo "  - Good for basic tasks, less RAM needed"
echo "  - ~2GB download, ~3GB RAM usage"
echo ""
read -p "Enter 1, 2, or 3: " choice

if [ "$choice" = "1" ]; then
    # Option 1: Llama 3.1 8B (RECOMMENDED)
    echo ""
    echo "1Ô∏è‚É£  Downloading Llama 3.1 8B Instruct (Q4_K_M)..."
    MODEL_FILE="Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"
    
    if [ ! -f "$MODEL_FILE" ]; then
        curl -L "https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf" \
             -o "$MODEL_FILE" \
             --progress-bar
        echo "‚úÖ Llama 3.1 8B downloaded"
    else
        echo "‚úÖ Llama 3.1 8B already exists"
    fi
    
    echo ""
    echo "üìä Storage used: ~4.9GB"
    echo "üíæ RAM required: ~5-6GB during inference"
    echo "üéØ Model: Llama 3.1 8B Instruct (excellent instruction following)"
    echo "üëÅÔ∏è  Vision: Use BLIP-2 (auto-downloads when needed)"
    
elif [ "$choice" = "2" ]; then
    # Option 2: Mistral 7B
    echo ""
    echo "2Ô∏è‚É£  Downloading Mistral 7B Instruct v0.2 (Q4_K_M)..."
    MODEL_FILE="mistral-7b-instruct-v0.2.Q4_K_M.gguf"
    
    if [ ! -f "$MODEL_FILE" ]; then
        curl -L "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf" \
             -o "$MODEL_FILE" \
             --progress-bar
        echo "‚úÖ Mistral 7B downloaded"
    else
        echo "‚úÖ Mistral 7B already exists"
    fi
    
    echo ""
    echo "üìä Storage used: ~4.4GB"
    echo "üíæ RAM required: ~5GB during inference"
    echo "üéØ Model: Mistral 7B Instruct v0.2 (fast & efficient)"
    echo "üëÅÔ∏è  Vision: Use BLIP-2 (auto-downloads when needed)"
    
elif [ "$choice" = "3" ]; then
    # Option 3: Llama 3.2 3B
    echo ""
    echo "3Ô∏è‚É£  Downloading Llama 3.2 3B Instruct (Q4_K_M)..."
    MODEL_FILE="llama-3.2-3b-instruct.Q4_K_M.gguf"
    
    if [ ! -f "$MODEL_FILE" ]; then
        curl -L "https://huggingface.co/lmstudio-community/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf" \
             -o "$MODEL_FILE" \
             --progress-bar
        echo "‚úÖ Llama 3.2 3B downloaded"
    else
        echo "‚úÖ Llama 3.2 3B already exists"
    fi
    
    echo ""
    echo "üìä Storage used: ~2GB"
    echo "üíæ RAM required: ~3GB during inference"
    echo "üéØ Model: Llama 3.2 3B Instruct (compact & fast)"
    echo "üëÅÔ∏è  Vision: Use BLIP-2 (auto-downloads when needed)"
    
else
    echo "Invalid choice. Run script again."
    exit 1
fi

echo ""
echo "2Ô∏è‚É£  Whisper Medium will auto-download on first use (~1.5GB)"
echo "3Ô∏è‚É£  Vision models (BLIP-2/YOLO if needed) will auto-download"
echo ""
echo "‚úÖ Model setup complete!"
echo ""
echo "Next steps:"
echo "1. Update backend/.env with model path"
echo "2. Test: source venv/bin/activate && python test_models.py"
